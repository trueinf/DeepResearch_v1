# Research Intelligence Map - Testing Guide

## ğŸ“‹ Testing Overview

This guide covers how to test the Research Intelligence Map feature and document your findings.

---

## ğŸ§ª Test Environment Setup

### Prerequisites
- [ ] Development server running (`npm run dev`)
- [ ] Supabase project configured
- [ ] Edge functions deployed (or ready to deploy)
- [ ] At least one completed research report available
- [ ] Browser with developer tools open (F12)

### Test Data Requirements
- [ ] Research report with rich content:
  - Executive summary
  - Multiple key findings
  - Detailed analysis
  - Insights
  - Conclusion
  - Sources

---

## ğŸ“ Test Cases

### Test Case 1: Access Intelligence Map from Report View

**Objective**: Verify the "View Intelligence Map" button appears and navigates correctly.

**Steps**:
1. Navigate to a completed research report (`/report/:id`)
2. Locate the "View Intelligence Map" button
3. Click the button
4. Verify navigation to `/map/:id`

**Expected Results**:
- âœ… Button is visible (purple gradient, Network icon)
- âœ… Button is positioned next to "Chat with Report" button
- âœ… Clicking navigates to Intelligence Map page
- âœ… URL changes to `/map/:id`

**Documentation**:
```
Test Date: [Date]
Tester: [Name]
Result: âœ… Pass / âŒ Fail
Notes: [Any observations]
Screenshot: [If applicable]
```

---

### Test Case 2: Graph Building (First Time)

**Objective**: Verify graph is built successfully when no graph exists.

**Steps**:
1. Navigate to `/map/:id` for a research report
2. Observe the loading state
3. Wait for graph building to complete
4. Check browser console for errors
5. Verify graph appears

**Expected Results**:
- âœ… Loading indicator shows "Building intelligence map..."
- âœ… No errors in browser console
- âœ… Graph visualization appears (nodes and edges visible)
- âœ… Stats show node count, relationship count, cluster count
- âœ… Process completes within 60-90 seconds

**Documentation**:
```
Test Date: [Date]
Research ID: [ID]
Build Time: [Seconds]
Nodes Created: [Number]
Relationships Created: [Number]
Clusters Found: [Number]
Errors: [None / List errors]
Console Logs: [Relevant logs]
```

---

### Test Case 3: Entity Extraction Accuracy

**Objective**: Verify entities are extracted correctly from research report.

**Steps**:
1. Build graph for a research report
2. Review extracted entities in the graph
3. Compare with original report content
4. Check entity types are correct
5. Verify entity labels match report content

**Expected Results**:
- âœ… Entities match content in report
- âœ… Entity types are appropriate (Person, Organization, Technology, etc.)
- âœ… No duplicate entities
- âœ… Entity labels are clear and meaningful
- âœ… Confidence scores are reasonable (0.5-1.0)

**Documentation**:
```
Test Date: [Date]
Research Topic: [Topic]
Sample Entities Found:
  - [Entity 1] (Type: [Type], Confidence: [Score])
  - [Entity 2] (Type: [Type], Confidence: [Score])
  - [Entity 3] (Type: [Type], Confidence: [Score])
Accuracy: âœ… Good / âš ï¸ Some issues / âŒ Poor
Issues Found: [List any incorrect extractions]
```

---

### Test Case 4: Relationship Extraction Accuracy

**Objective**: Verify relationships between entities are correctly identified.

**Steps**:
1. Build graph for a research report
2. Review relationships in the graph
3. Verify relationship types are appropriate
4. Check relationship evidence makes sense
5. Verify source and target entities are correct

**Expected Results**:
- âœ… Relationships are logical and accurate
- âœ… Relationship types match the connection (INFLUENCES, CAUSES, etc.)
- âœ… Evidence text supports the relationship
- âœ… No circular or invalid relationships
- âœ… Strength scores are reasonable

**Documentation**:
```
Test Date: [Date]
Sample Relationships Found:
  - [Entity A] --[INFLUENCES]--> [Entity B]
    Evidence: [Evidence text]
    Confidence: [Score]
    Strength: [Score]
  - [Entity C] --[CAUSES]--> [Entity D]
    Evidence: [Evidence text]
    Confidence: [Score]
    Strength: [Score]
Accuracy: âœ… Good / âš ï¸ Some issues / âŒ Poor
Issues Found: [List any incorrect relationships]
```

---

### Test Case 5: Graph Visualization - Basic Interactions

**Objective**: Verify basic graph visualization interactions work.

**Steps**:
1. Load graph visualization
2. Test zoom in (button or mouse wheel)
3. Test zoom out (button or mouse wheel)
4. Test pan/drag (click and drag background)
5. Test node drag (click and drag a node)
6. Test hover on node (shows tooltip)
7. Test click on node (selects node, shows details)

**Expected Results**:
- âœ… Zoom in/out buttons work
- âœ… Mouse wheel zoom works
- âœ… Panning works smoothly
- âœ… Nodes can be dragged
- âœ… Hover shows tooltip with node info
- âœ… Click selects node and shows details panel
- âœ… No lag or performance issues

**Documentation**:
```
Test Date: [Date]
Zoom: âœ… Works / âŒ Issues
Pan: âœ… Works / âŒ Issues
Drag Nodes: âœ… Works / âŒ Issues
Hover: âœ… Works / âŒ Issues
Click: âœ… Works / âŒ Issues
Performance: âœ… Smooth / âš ï¸ Some lag / âŒ Poor
Notes: [Any issues observed]
```

---

### Test Case 6: Filter by Entity Type

**Objective**: Verify filtering by entity type works correctly.

**Steps**:
1. Load graph visualization
2. Note total number of nodes
3. Click "All Types" filter (should show all)
4. Click a specific entity type filter (e.g., "Technology")
5. Verify only that type is shown
6. Click another type
7. Verify filter changes
8. Click "All Types" again

**Expected Results**:
- âœ… "All Types" shows all nodes
- âœ… Selecting a type filters correctly
- âœ… Only nodes of selected type are visible
- âœ… Relationships update to show only relevant connections
- âœ… Filter button highlights when active
- âœ… Switching filters works smoothly

**Documentation**:
```
Test Date: [Date]
Total Nodes: [Number]
Filter Tests:
  - All Types: âœ… Shows [Number] nodes
  - Technology: âœ… Shows [Number] nodes
  - Organization: âœ… Shows [Number] nodes
  - Person: âœ… Shows [Number] nodes
  - [Other types tested]
Issues: [Any filtering problems]
```

---

### Test Case 7: Cluster Highlighting

**Objective**: Verify cluster detection and highlighting works.

**Steps**:
1. Load graph visualization
2. Check clusters panel in sidebar
3. Note number of clusters
4. Click on a cluster
5. Verify cluster nodes are highlighted
6. Verify other nodes are dimmed
7. Click another cluster
8. Verify highlight changes
9. Click same cluster again (should deselect)

**Expected Results**:
- âœ… Clusters are detected and listed
- âœ… Cluster themes are meaningful
- âœ… Clicking cluster highlights nodes
- âœ… Non-cluster nodes are dimmed
- âœ… Cluster info shows node count
- âœ… Deselecting works (click again)

**Documentation**:
```
Test Date: [Date]
Clusters Found: [Number]
Cluster Details:
  - Cluster 1: [Theme] - [Number] nodes âœ… Highlights correctly
  - Cluster 2: [Theme] - [Number] nodes âœ… Highlights correctly
  - Cluster 3: [Theme] - [Number] nodes âœ… Highlights correctly
Issues: [Any highlighting problems]
```

---

### Test Case 8: Node Details Panel

**Objective**: Verify node details are displayed correctly when selected.

**Steps**:
1. Load graph visualization
2. Click on a node
3. Verify node details panel appears in sidebar
4. Check displayed information:
   - Label
   - Type
   - Description
   - Confidence score
5. Click another node
6. Verify details update
7. Click on background (empty area)
8. Verify details panel clears

**Expected Results**:
- âœ… Details panel appears when node clicked
- âœ… All information is displayed correctly
- âœ… Confidence score is shown as percentage
- âœ… Description is readable
- âœ… Clicking another node updates panel
- âœ… Clicking background clears selection

**Documentation**:
```
Test Date: [Date]
Node Details Test:
  - Node: [Entity Name]
  - Label: âœ… Correct
  - Type: âœ… Correct
  - Description: âœ… Present / âŒ Missing
  - Confidence: âœ… [Score]%
  - Citations: âœ… [Number] / âŒ Missing
Issues: [Any display problems]
```

---

### Test Case 9: Centrality Rankings

**Objective**: Verify most influential nodes are ranked correctly.

**Steps**:
1. Load graph visualization
2. Check "Most Influential" panel in sidebar
3. Verify top 5 nodes are listed
4. Check scores are displayed
5. Verify nodes in ranking match graph nodes
6. Compare visual node sizes with rankings

**Expected Results**:
- âœ… Top 5 nodes are listed
- âœ… Scores are displayed (numeric)
- âœ… Nodes match graph entities
- âœ… Higher scores = more connections (generally)
- âœ… Node sizes correlate with centrality

**Documentation**:
```
Test Date: [Date]
Top 5 Influential Nodes:
  1. [Node Name] - Score: [Number]
  2. [Node Name] - Score: [Number]
  3. [Node Name] - Score: [Number]
  4. [Node Name] - Score: [Number]
  5. [Node Name] - Score: [Number]
Accuracy: âœ… Makes sense / âš ï¸ Questionable / âŒ Wrong
Notes: [Observations]
```

---

### Test Case 10: Export Functionality

**Objective**: Verify graph export works correctly.

**Steps**:
1. Load graph visualization
2. Click "Export Graph" button (download icon)
3. Verify file downloads
4. Check file name format: `research_map_[id]_[timestamp].json`
5. Open downloaded JSON file
6. Verify structure:
   - `topic` field
   - `nodes` array
   - `relationships` array
   - `clusters` array
   - `centrality` array
   - `exportedAt` timestamp

**Expected Results**:
- âœ… File downloads successfully
- âœ… File name is correct format
- âœ… JSON is valid and parseable
- âœ… All graph data is included
- âœ… Data structure matches expected format

**Documentation**:
```
Test Date: [Date]
Export Test:
  - File Name: âœ… Correct format
  - File Size: [Bytes/KB]
  - JSON Valid: âœ… Yes / âŒ No
  - Contains Nodes: âœ… [Number]
  - Contains Relationships: âœ… [Number]
  - Contains Clusters: âœ… [Number]
  - Contains Centrality: âœ… Yes / âŒ No
Issues: [Any export problems]
```

---

### Test Case 11: Graph Rebuild

**Objective**: Verify graph can be rebuilt/refreshed.

**Steps**:
1. Load existing graph
2. Click "Rebuild Graph" button (refresh icon)
3. Verify loading state appears
4. Wait for rebuild to complete
5. Verify graph updates
6. Check if data changed (if report was updated)

**Expected Results**:
- âœ… Rebuild button is visible
- âœ… Loading state shows during rebuild
- âœ… Graph rebuilds successfully
- âœ… No errors occur
- âœ… Graph updates if report changed

**Documentation**:
```
Test Date: [Date]
Rebuild Test:
  - Button Visible: âœ… Yes
  - Loading State: âœ… Works
  - Rebuild Time: [Seconds]
  - Success: âœ… Yes / âŒ No
  - Data Updated: âœ… Yes / âŒ No
Issues: [Any problems]
```

---

### Test Case 12: Error Handling

**Objective**: Verify error handling works gracefully.

**Test Scenarios**:

#### 12a: Missing Report Data
**Steps**:
1. Navigate to `/map/:id` with invalid ID
2. Verify error message appears
3. Check error is user-friendly

**Expected**: Clear error message, retry option

#### 12b: API Failure
**Steps**:
1. Disable network (or block API calls)
2. Try to build graph
3. Verify error handling

**Expected**: Error message, no crash

#### 12c: Empty Report
**Steps**:
1. Create report with minimal content
2. Try to build graph
3. Verify handling

**Expected**: Graceful handling, informative message

**Documentation**:
```
Test Date: [Date]
Error Handling Tests:
  - Invalid ID: âœ… Handled / âŒ Crashes
  - API Failure: âœ… Handled / âŒ Crashes
  - Empty Report: âœ… Handled / âŒ Crashes
Error Messages: âœ… Clear / âŒ Confusing
User Experience: âœ… Good / âŒ Poor
```

---

### Test Case 13: Performance Testing

**Objective**: Verify performance is acceptable.

**Steps**:
1. Build graph for large report (many findings)
2. Measure build time
3. Test visualization performance:
   - Zoom in/out speed
   - Pan smoothness
   - Node drag responsiveness
4. Check memory usage (browser dev tools)
5. Test with multiple graphs in same session

**Expected Results**:
- âœ… Build time < 90 seconds for typical report
- âœ… Visualization is smooth (60fps)
- âœ… No memory leaks
- âœ… No browser crashes

**Documentation**:
```
Test Date: [Date]
Performance Metrics:
  - Build Time: [Seconds]
  - Nodes: [Number]
  - Relationships: [Number]
  - Visualization FPS: [FPS]
  - Memory Usage: [MB]
  - Browser: [Chrome/Firefox/etc]
Performance: âœ… Good / âš ï¸ Acceptable / âŒ Poor
Issues: [Performance problems]
```

---

### Test Case 14: Neo4j Integration (If Configured)

**Objective**: Verify Neo4j persistence works.

**Steps**:
1. Build graph (should save to Neo4j)
2. Close browser tab
3. Reopen same graph URL
4. Verify graph loads from Neo4j (faster)
5. Check Neo4j database for nodes/relationships
6. Verify researchId isolation

**Expected Results**:
- âœ… Graph saves to Neo4j
- âœ… Graph loads from Neo4j on revisit
- âœ… Load time is faster (no rebuild)
- âœ… Data persists correctly
- âœ… Research isolation works

**Documentation**:
```
Test Date: [Date]
Neo4j Test:
  - Connection: âœ… Working / âŒ Failed
  - Save: âœ… Success / âŒ Failed
  - Load: âœ… Success / âŒ Failed
  - Load Time: [Seconds] (should be < 5s)
  - Data Persistence: âœ… Correct / âŒ Missing
  - Isolation: âœ… Works / âŒ Issues
```

---

## ğŸ“Š Test Results Template

### Test Session Summary

```
===========================================
RESEARCH INTELLIGENCE MAP - TEST RESULTS
===========================================

Test Date: [Date]
Tester: [Name]
Environment: [Development/Staging/Production]
Browser: [Chrome/Firefox/Safari] [Version]
Node Version: [Version]
Supabase Project: [Project Name]

OVERALL STATUS: âœ… PASS / âš ï¸ PARTIAL / âŒ FAIL

===========================================
TEST RESULTS
===========================================

Test Case 1: Access Intelligence Map
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 2: Graph Building
  Status: âœ… / âŒ
  Build Time: [Seconds]
  Nodes: [Number]
  Relationships: [Number]
  Notes: [Notes]

Test Case 3: Entity Extraction
  Status: âœ… / âŒ
  Accuracy: [Good/Fair/Poor]
  Notes: [Notes]

Test Case 4: Relationship Extraction
  Status: âœ… / âŒ
  Accuracy: [Good/Fair/Poor]
  Notes: [Notes]

Test Case 5: Basic Interactions
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 6: Filtering
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 7: Cluster Highlighting
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 8: Node Details
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 9: Centrality Rankings
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 10: Export
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 11: Rebuild
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 12: Error Handling
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 13: Performance
  Status: âœ… / âŒ
  Notes: [Notes]

Test Case 14: Neo4j Integration
  Status: âœ… / âŒ / N/A (not configured)
  Notes: [Notes]

===========================================
ISSUES FOUND
===========================================

[Priority] [Issue Description]
  - Steps to reproduce
  - Expected behavior
  - Actual behavior
  - Screenshots/logs

===========================================
RECOMMENDATIONS
===========================================

[Any recommendations for improvements]

===========================================
```

---

## ğŸ” Testing Checklist

### Pre-Testing
- [ ] Development server running
- [ ] Supabase configured
- [ ] Edge functions deployed
- [ ] Test research report available
- [ ] Browser dev tools open

### Core Functionality
- [ ] Can access Intelligence Map from report
- [ ] Graph builds successfully
- [ ] Entities extracted correctly
- [ ] Relationships extracted correctly
- [ ] Visualization displays correctly

### Interactions
- [ ] Zoom works
- [ ] Pan works
- [ ] Node drag works
- [ ] Hover shows tooltip
- [ ] Click selects node
- [ ] Filters work
- [ ] Clusters highlight
- [ ] Export works

### Edge Cases
- [ ] Handles missing data
- [ ] Handles API errors
- [ ] Handles empty reports
- [ ] Handles large graphs
- [ ] Handles slow network

### Performance
- [ ] Build time acceptable
- [ ] Visualization smooth
- [ ] No memory leaks
- [ ] No crashes

---

## ğŸ“¸ Screenshot Guidelines

Take screenshots for:
1. âœ… Successful graph build
2. âœ… Graph visualization
3. âœ… Filtered views
4. âœ… Cluster highlighting
5. âœ… Node details panel
6. âœ… Error states
7. âœ… Export confirmation

**Screenshot Naming**: `test_[testcase]_[date]_[timestamp].png`

---

## ğŸ› Bug Report Template

```
BUG REPORT
==========

Title: [Brief description]
Severity: [Critical/High/Medium/Low]
Test Case: [Which test case]
Date: [Date]
Reporter: [Name]

DESCRIPTION
-----------
[Detailed description of the issue]

STEPS TO REPRODUCE
------------------
1. [Step 1]
2. [Step 2]
3. [Step 3]

EXPECTED BEHAVIOR
-----------------
[What should happen]

ACTUAL BEHAVIOR
---------------
[What actually happens]

ENVIRONMENT
-----------
- Browser: [Browser + Version]
- OS: [Operating System]
- Research ID: [ID if applicable]
- Graph Stats: [Nodes/Relationships if applicable]

SCREENSHOTS/LOGS
----------------
[Attach screenshots or console logs]

ADDITIONAL NOTES
---------------
[Any other relevant information]
```

---

## âœ… Sign-Off

After completing all tests:

```
TESTING COMPLETE
================

Date: [Date]
Tester: [Name]
Status: âœ… Ready for Production / âš ï¸ Needs Fixes / âŒ Not Ready

Critical Issues: [Number]
High Issues: [Number]
Medium Issues: [Number]
Low Issues: [Number]

Approved By: [Name]
Date: [Date]
```

---

## ğŸ“ Notes

- Test with different research topics (technology, healthcare, business, etc.)
- Test with reports of varying sizes (small, medium, large)
- Test on different browsers
- Test on different screen sizes
- Document any unexpected behaviors
- Keep detailed logs of API calls and responses
- Note any performance issues

---

**Last Updated**: [Date]  
**Version**: 1.0

